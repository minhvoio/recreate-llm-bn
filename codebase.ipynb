{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7268afb",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9e4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemma3n:e4b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4bc15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "def answer_this_prompt(prompt, stream=False, model=MODEL, temperature=0, format=None):\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": 50, # only when stream = False work\n",
    "        \"format\": format\n",
    "    }\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    endpoint = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    # Send the POST request with streaming enabled\n",
    "    with requests.post(endpoint, headers=headers, json=payload, stream=True) as response:\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                # Process the response incrementally\n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines(decode_unicode=True):\n",
    "                    if line.strip():  # Skip empty lines\n",
    "                        response_json = json.loads(line)\n",
    "                        chunk = response_json.get(\"response\", \"\")\n",
    "                        full_response += chunk\n",
    "                        \n",
    "                        # Render the response as Markdown\n",
    "                        if stream:\n",
    "                            clear_output(wait=True)\n",
    "                            display(Markdown(full_response))\n",
    "                        \n",
    "                return full_response\n",
    "            except json.JSONDecodeError as e:\n",
    "                return \"Failed to parse JSON: \" + str(e)\n",
    "        else:\n",
    "            return \"Failed to retrieve response: \" + str(response.status_code)\n",
    "        \n",
    "def multiple_answer_this_prompt(prompt, stream=False, model=MODEL, temperature=0, format=None, n_answers=1):\n",
    "    answers = []\n",
    "    for _ in range(n_answers):\n",
    "        answer = answer_this_prompt(prompt, stream=stream, model=model, temperature=temperature, format=format)\n",
    "        answers.append(answer)\n",
    "    return answers\n",
    "\n",
    "# Example usage\n",
    "# ans = answer_this_prompt(\"What is the Big Bang theory?\", stream=True)\n",
    "# print(\"------------------------\")\n",
    "# print(type(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3fff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asia', 'tub'), ('tub', 'either'), ('smoke', 'lung'), ('smoke', 'bronc'), ('lung', 'either'), ('bronc', 'dysp'), ('either', 'xray'), ('either', 'dysp')]\n",
      "+-----------+-------------+\n",
      "| lung      |   phi(lung) |\n",
      "+===========+=============+\n",
      "| lung(yes) |      0.3715 |\n",
      "+-----------+-------------+\n",
      "| lung(no)  |      0.6285 |\n",
      "+-----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.utils import get_example_model\n",
    "\n",
    "asia = get_example_model('asia')\n",
    "# print(asia.nodes())\n",
    "print(asia.edges())\n",
    "# print(asia.get_cpds())\n",
    "\n",
    "def query_bn(bn, query_variables, evidence):\n",
    "    \"\"\"\n",
    "    Perform exact inference on a Bayesian Network (BN) given the query variables and evidence.\n",
    "    \n",
    "    Args:\n",
    "    - bn: A Bayesian Network object (pgmpy.models.BayesianNetwork).\n",
    "    - query_variables: A list of strings specifying the query variables.\n",
    "    - evidence: A dictionary where keys are strings of the evidence variables and values are the observed states.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of tuples where each tuple contains the state and probability of the query variable.\n",
    "    \"\"\"\n",
    "    # Perform exact inference using Variable Elimination\n",
    "    from pgmpy.inference import VariableElimination\n",
    "    inference = VariableElimination(bn)\n",
    "    result = inference.query(variables=query_variables, evidence=evidence)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "query_variables = ['lung']\n",
    "evidence = {'asia': 'yes', 'xray': 'yes'}\n",
    "result = query_bn(asia, query_variables, evidence)\n",
    "rs1 = str(result)\n",
    "print(rs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0216db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_intervention(model, variable, forced_value_index, state_names):\n",
    "    import copy\n",
    "    from pgmpy.factors.discrete import TabularCPD\n",
    "    \n",
    "    intervened = copy.deepcopy(model)\n",
    "    intervened.remove_cpds(variable)\n",
    "    for parent in intervened.get_parents(variable):\n",
    "        intervened.remove_edge(parent, variable)\n",
    "    \n",
    "    values = [[1.0 if i == forced_value_index else 0.0] for i in range(len(state_names))]\n",
    "    \n",
    "    cpd = TabularCPD(\n",
    "        variable=variable,\n",
    "        variable_card=len(state_names),\n",
    "        values=values,\n",
    "        state_names={variable: state_names}\n",
    "    )\n",
    "    intervened.add_cpds(cpd)\n",
    "    assert intervened.check_model()\n",
    "    intervened_variable = f\"\"\"'{variable}': '{state_names[forced_value_index]}'\"\"\"\n",
    "    return intervened, intervened_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ddf9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asia', 'tub'), ('tub', 'either'), ('smoke', 'lung'), ('smoke', 'bronc'), ('lung', 'either'), ('bronc', 'dysp'), ('either', 'xray'), ('either', 'dysp')]\n",
      "+-----------+-------------+\n",
      "| lung      |   phi(lung) |\n",
      "+===========+=============+\n",
      "| lung(yes) |      0.4903 |\n",
      "+-----------+-------------+\n",
      "| lung(no)  |      0.5097 |\n",
      "+-----------+-------------+\n",
      "'asia': 'no'\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "do_model, itv_var = do_intervention(asia, 'asia', forced_value_index=1, state_names=['yes', 'no'])\n",
    "print(do_model.edges())\n",
    "inference2 = VariableElimination(do_model)\n",
    "rs2 = str(inference2.query(variables=['lung'], evidence={'xray': 'yes'}))\n",
    "print(rs2)\n",
    "print(itv_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac1832",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1babed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN = \"\"\"[('asia', 'tub'), ('tub', 'either'), ('smoke', 'lung'), ('smoke', 'bronc'), ('lung', 'either'), ('bronc', 'dysp'), ('either', 'xray'), ('either', 'dysp')]\"\"\"\n",
    "BN = asia.edges()\n",
    "QUERY = ''\n",
    "BN_LOG = \"\"\"\n",
    "Bayesian Network Analysis Log\n",
    "------------------------------\n",
    "Network Structure: {BN}\n",
    "Observed Evidence: {evidence}\n",
    "Target Variables: {query_variables}\n",
    "\n",
    "Initial Query Result (No Intervention): {rs1}\n",
    "\n",
    "Intervention Applied: {itv_var}\n",
    "Query Result After Intervention: {rs2} \n",
    "\n",
    "Notes:\n",
    "- The purpose of this analysis is to observe how the intervention affects the target variables.\n",
    "- The changes in query results are expected to reflect causal effects, assuming the network structure and CPDs are correct.\n",
    "- Interpretations should be based on the structural dependencies encoded in the Bayesian Network.\n",
    "\"\"\"\n",
    "\n",
    "EXPL_PROMPT = \"\"\"Generate an explanation of the information in the following Bayesian Network (BN) Analysis Log.\n",
    "\n",
    "Only respond with the explanation, do not include any extraneous text.\n",
    "\n",
    "BN Analysis Log:\n",
    "\n",
    "{BN_LOG}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f6d16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This Bayesian Network (BN) analysis explores the relationship between 'asia', 'smoke', 'lung', 'bronc', 'dysp', and 'xrray', with the goal of understanding how intervening on 'asia' influences the probability of 'lung' disease. \n",
       "\n",
       "The network structure indicates that 'asia' influences 'tub', which in turn influences 'either' (representing either 'lung' or 'bronc' disease). 'Smoke' influences both 'lung' and 'bronc' diseases. 'Lung' and 'bronc' both influence 'dysp' (dyspnea).  'Either' influences 'xrray'. The target variable is 'lung'.\n",
       "\n",
       "Initially, without any intervention, the probability of 'lung' is 37.15% and the probability of 'no lung' is 62.85%.\n",
       "\n",
       "An intervention is applied to 'asia', setting it to 'no'. After this intervention, the probability of 'lung' increases to 49.03% and the probability of 'no lung' decreases to 50.97%.\n",
       "\n",
       "The analysis suggests that intervening to negate 'asia' (presumably a risk factor related to Asian populations) increases the probability of 'lung' disease. This change in probability is attributed to the causal relationships encoded in the Bayesian Network, specifically the influence of 'asia' on 'tub', which subsequently affects the probability of 'lung' disease. The intervention effectively modifies the path leading to 'lung' through 'tub' and 'either'.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "bg_log = BN_LOG.format(BN=BN, evidence=evidence, query_variables=query_variables, rs1=rs1, itv_var=itv_var, rs2=rs2)\n",
    "# print(bg_log)\n",
    "gemma3n_answer = answer_this_prompt(EXPL_PROMPT.format(BN_LOG=bg_log), stream=True, model=MODEL)\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4c9be",
   "metadata": {},
   "source": [
    "# Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "918a6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUIZ_PROMPT = \"\"\"\n",
    "Generate a multiple-choice quiz based on the information in the following Bayesian Network (BN) Analysis Log.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "1. What evidence make the most changed the target variable?\n",
    "A. asia\n",
    "B. tub\n",
    "C. smoke\n",
    "D. lung\n",
    "\n",
    "2. What would happen to the target variable if we change the observation of the evidence?\n",
    "A. It would increase\n",
    "B. It would decrease\n",
    "C. It would remain the same\n",
    "D. It would become undefined\n",
    "\n",
    "3. What would happen to the target variable if we intervene the evidence variable?\n",
    "A. It would increase\n",
    "B. It would decrease\n",
    "C. It would remain the same\n",
    "D. It would become undefined\n",
    "\n",
    "4. What would happen to 'lung' if we intervene on 'xray'?\n",
    "A. It would increase\n",
    "B. It would decrease\n",
    "C. It would remain the same\n",
    "D. It would become undefined\n",
    "\n",
    "```\n",
    "\n",
    "===== ANSWERS =====\n",
    "1. C\n",
    "2. A\n",
    "3. A\n",
    "4. A\n",
    "```\n",
    "\n",
    "Limit the length of the quiz to the top 10 most relevant questions for BN explaination about the analysis log.\n",
    "\n",
    "BN Analysis Log:\n",
    "\n",
    "{BN_LOG}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f20a022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from random import shuffle\n",
    "\n",
    "class Question(BaseModel):\n",
    "  text: str\n",
    "  options: list[str]\n",
    "  answer: int\n",
    "\n",
    "  def shuffle_options(self) -> None:\n",
    "    correct = self.options[self.answer]\n",
    "\n",
    "    shuffled = self.options.copy()\n",
    "    shuffle(shuffled)\n",
    "\n",
    "    self.options = shuffled\n",
    "    self.answer = shuffled.index(correct)\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "    output = [self.text]\n",
    "\n",
    "    for i, option in enumerate(self.options):\n",
    "      output.append(f\"{chr(65+i)}. {option}\")\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "  \n",
    "class Quiz(BaseModel):\n",
    "  questions: list[Question]\n",
    "\n",
    "  def shuffle_all_questions(self) -> None:\n",
    "    for question in self.questions:\n",
    "      question.shuffle_options()\n",
    "\n",
    "  def __str__(self):\n",
    "    output = []\n",
    "\n",
    "    for i, question in enumerate(self.questions):\n",
    "      output.append(f\"\\nQuestion {i+1}:\")\n",
    "      output.append(str(question))\n",
    "\n",
    "    return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b660d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quiz(BN_LOG: str):\n",
    "  prompt = QUIZ_PROMPT.format(BN_LOG=BN_LOG)\n",
    "  ans = answer_this_prompt(prompt, format=Quiz.model_json_schema())\n",
    "  quiz = Quiz.model_validate_json(ans)\n",
    "  quiz.shuffle_all_questions()\n",
    "  return quiz\n",
    "\n",
    "quiz = create_quiz(bg_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fec94032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1:\n",
      "What evidence was observed in this analysis?\n",
      "A. asia: no, xray: no\n",
      "B. asia: yes, xray: yes\n",
      "C. asia: yes, xray: no\n",
      "D. asia: no, xray: yes\n",
      "\n",
      "Question 2:\n",
      "What is the target variable in this analysis?\n",
      "A. lung\n",
      "B. bronc\n",
      "C. asia\n",
      "D. tub\n",
      "\n",
      "Question 3:\n",
      "What is the initial probability of 'lung(yes)' before any intervention?\n",
      "A. 0.6285\n",
      "B. 0.5097\n",
      "C. 0.3715\n",
      "D. 0.5\n",
      "\n",
      "Question 4:\n",
      "What is the probability of 'lung(yes)' after intervening on 'asia' to 'no'?\n",
      "A. 0.6285\n",
      "B. 0.4903\n",
      "C. 0.3715\n",
      "D. 0.5097\n",
      "\n",
      "Question 5:\n",
      "How does intervening on 'asia' to 'no' affect the probability of 'lung(no)'?\n",
      "A. It becomes undefined\n",
      "B. It increases\n",
      "C. It remains the same\n",
      "D. It decreases\n",
      "\n",
      "Question 6:\n",
      "The analysis aims to observe the effect of what on the target variable?\n",
      "A. Indirect effect\n",
      "B. Causal effect\n",
      "C. Correlation\n",
      "D. Direct effect\n",
      "\n",
      "Question 7:\n",
      "According to the notes, what is assumed to be correct for the interpretations?\n",
      "A. The network structure and Conditional Probability Distributions (CPDs)\n",
      "B. The intervention\n",
      "C. The observed evidence\n",
      "D. The target variable\n",
      "\n",
      "Question 8:\n",
      "What is the relationship between 'asia' and 'tub' in the Bayesian Network?\n",
      "A. asia and tub are independent\n",
      "B. asia -> tub\n",
      "C. tub -> asia\n",
      "D. tub -> asia and asia -> tub\n",
      "\n",
      "Question 9:\n",
      "What is the relationship between 'smoke' and 'lung' in the Bayesian Network?\n",
      "A. lung -> smoke and smoke -> lung\n",
      "B. lung -> smoke\n",
      "C. smoke -> lung\n",
      "D. smoke and lung are independent\n",
      "\n",
      "Question 10:\n",
      "What is the relationship between 'either' and 'xrray' in the Bayesian Network?\n",
      "A. either -> xray\n",
      "B. either and xray are independent\n",
      "C. xrray -> either\n",
      "D. xrray -> either and either -> xray\n"
     ]
    }
   ],
   "source": [
    "print(quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6aeca3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_to_index = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "index_to_letter = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "\n",
    "TAKE_QUIZ_PROMPT = \"\"\"Use the provided Bayesian Network Explanation of a Bayesian Network Analysis Log\n",
    "to answer the following quiz.\n",
    "\n",
    "Quiz:\n",
    "\n",
    "{quiz}\n",
    "\n",
    "Bayesian Network Explanation:\n",
    "\n",
    "{bn_explanation}\n",
    "\n",
    "Respond with just a list of answers and no additional text, \n",
    "for example:\n",
    "\n",
    "[A, D, C, B, B, C, D, A, A, B]\n",
    "\n",
    "You must provide an answer for all 10 questions. \n",
    "If you don't know the answer, answer with \"0\" for that question. \n",
    "Example:\n",
    "\n",
    "[A, D, 0, B, B, C, D, A, A, B]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7101dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_quiz(quiz: Quiz, bn_explanation: str):\n",
    "    question_strs = []\n",
    "    for question in quiz.questions:\n",
    "        question_str = question.text\n",
    "        for i, option in enumerate(question.options):\n",
    "            letter = index_to_letter[i]\n",
    "            question_str += f\"\\n{letter}. {option}\"\n",
    "        question_strs.append(question_str)\n",
    "    quiz_str = \"\\n\\n\".join(question_strs)\n",
    "\n",
    "    prompt = TAKE_QUIZ_PROMPT.format(quiz=quiz_str, bn_explanation=bn_explanation)\n",
    "    res_str = answer_this_prompt(prompt)\n",
    "    ans = res_str.strip(\"[]\").split(\", \")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a4f7c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'A', 'C', 'B', 'D', 'B', 'C', 'C', 'B', 'D']\n"
     ]
    }
   ],
   "source": [
    "answer = take_quiz(quiz, gemma3n_answer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4dd3ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_quiz_answer(answers: list[str], quiz: Quiz):\n",
    "  assert len(answers) == len(quiz.questions), \"Number of answers must match number of questions\"\n",
    "  total = len(answers)\n",
    "\n",
    "  correct = 0\n",
    "  for answer, question in zip(answers, quiz.questions):\n",
    "    expected_answer = index_to_letter[question.answer]\n",
    "    if answer == expected_answer:\n",
    "      correct += 1\n",
    "\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40f22681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(answer))  \n",
    "print(len(quiz.questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "829e5614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "score = score_quiz_answer(answer, quiz)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ca39ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "def compute_advantages(rewards: list):\n",
    "    rewards = np.array(rewards)\n",
    "\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "\n",
    "    if std_reward == 0:\n",
    "        return [0] * len(rewards)\n",
    "    \n",
    "    advantages = (rewards - mean_reward) / std_reward\n",
    "    return advantages.tolist()\n",
    "\n",
    "def print_quiz_table(all_answers, rewards):\n",
    "    advantages = compute_advantages(rewards)\n",
    "    length = len(all_answers)\n",
    "    elems = list(zip(range(length), rewards, advantages))\n",
    "\n",
    "    headers = [\"Index\", \"Reward\", \"Advantage\"]\n",
    "    table = tabulate(elems, headers=headers, tablefmt=\"grid\").split(\"\\n\")\n",
    "    for row in table:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5ece43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_explanations = multiple_answer_this_prompt(EXPL_PROMPT.format(BN_LOG=bg_log), model=MODEL, n_answers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e557e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers = []\n",
    "quiz_rewards = []\n",
    "\n",
    "for bn_expl in bn_explanations:\n",
    "  answer = take_quiz(quiz, bn_expl)\n",
    "  all_answers.append(answer)\n",
    "  quiz_rewards.append(score_quiz_answer(answer, quiz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b0769f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+\n",
      "|   Index |   Reward |   Advantage |\n",
      "+=========+==========+=============+\n",
      "|       0 |      0.6 |           1 |\n",
      "+---------+----------+-------------+\n",
      "|       1 |      0.5 |          -1 |\n",
      "+---------+----------+-------------+\n",
      "|       2 |      0.5 |          -1 |\n",
      "+---------+----------+-------------+\n",
      "|       3 |      0.6 |           1 |\n",
      "+---------+----------+-------------+\n",
      "|       4 |      0.5 |          -1 |\n",
      "+---------+----------+-------------+\n",
      "|       5 |      0.5 |          -1 |\n",
      "+---------+----------+-------------+\n",
      "|       6 |      0.6 |           1 |\n",
      "+---------+----------+-------------+\n",
      "|       7 |      0.5 |          -1 |\n",
      "+---------+----------+-------------+\n",
      "|       8 |      0.6 |           1 |\n",
      "+---------+----------+-------------+\n",
      "|       9 |      0.6 |           1 |\n",
      "+---------+----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "print_quiz_table(all_answers, quiz_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28a965f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'A', 'B', 'B', 'D', 'B', 'C', 'C', 'C', 'B']\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "draft_answer = take_quiz(quiz, BN_LOG)\n",
    "draft_score = score_quiz_answer(draft_answer, quiz)\n",
    "\n",
    "print(draft_answer)\n",
    "print(draft_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147cb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answers_get_reward(quiz: Quiz, bn_explanations: list[str]) -> list[float]:\n",
    "  quiz_rewards = []\n",
    "  for bn_expl in bn_explanations:\n",
    "    answer = take_quiz(quiz, bn_expl)\n",
    "    quiz_rewards.append(score_quiz_answer(answer, quiz))\n",
    "  return quiz_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6091dea",
   "metadata": {},
   "source": [
    "# GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367d0fa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth currently only works on NVIDIA GPUs and Intel GPUs.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m max_seq_length = \u001b[32m2048\u001b[39m \u001b[38;5;66;03m# Can increase for longer reasoning traces\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm-bn/lib/python3.12/site-packages/unsloth/__init__.py:92\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m DEVICE_TYPE : \u001b[38;5;28mstr\u001b[39m = \u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Reduce VRAM usage by reducing fragmentation\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# And optimize pinning of memory\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DEVICE_TYPE == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mUNSLOTH_VLLM_STANDBY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)==\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm-bn/lib/python3.12/site-packages/unsloth/__init__.py:90\u001b[39m, in \u001b[36mget_device_type\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m torch.xpu.is_available():\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.01, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe47dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_length = 1024\n",
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-6,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 100,\n",
    "    save_steps = 100,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        check_answers_get_reward,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7329b31",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437936fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Why intervene asia change lung\"\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b96837",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00009b36",
   "metadata": {},
   "source": [
    "Verify LoRA is actually trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"grpo_saved_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "    # Verify both A and B are non zero\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "        assert(n_zeros.item() != tensor.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3ae1b",
   "metadata": {},
   "source": [
    "Now we load the LoRA and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695fbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": BN_LOG},\n",
    "    {\"role\": \"user\",   \"content\": 'What is the probability of having lung cancer if do go to asia'},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
